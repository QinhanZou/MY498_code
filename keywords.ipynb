{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read preprocessed data\n",
    "df = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 19631\n"
     ]
    }
   ],
   "source": [
    "# totoal number of rows\n",
    "print('Total number of rows:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Article Title', 'Source Title', 'Language',\n",
       "       'Times Cited, All Databases', 'Highly Cited Status', 'Hot Paper Status',\n",
       "       'Publication Year', 'Decade', 'Group', 'WoS Categories new',\n",
       "       'Research Areas new', 'Keywords Plus lemmatized',\n",
       "       'Author Keywords lemmatized', 'All Keywords', 'Addresses new',\n",
       "       'Affiliations new', 'Abstract lemmatized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['economic optimization machine learning output', 'payment fraud risk management', 'integration of machine learning and statistical risk modelling', 'banking fraud', 'ensemble model', 'anomaly detection', 'system']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['All Keywords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['All Keywords'] = df['All Keywords'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46038"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = pd.Series([word for keywords in df['All Keywords'] for word in keywords]).value_counts()\n",
    "word_freq = list(word_freq.items())\n",
    "word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine learning', 8757),\n",
       " ('model', 1830),\n",
       " ('artificial intelligence', 1705),\n",
       " ('classification', 1310),\n",
       " ('neural network', 930),\n",
       " ('performance', 853),\n",
       " ('impact', 849),\n",
       " ('prediction', 844),\n",
       " ('random forest', 815),\n",
       " ('big data', 809),\n",
       " ('deep learning', 758),\n",
       " ('support vector machine', 614),\n",
       " ('algorithm', 611),\n",
       " ('social medium', 580),\n",
       " ('natural language processing', 576),\n",
       " ('regression', 563),\n",
       " ('system', 561),\n",
       " ('risk', 550),\n",
       " ('selection', 529),\n",
       " ('information', 516),\n",
       " ('behavior', 434),\n",
       " ('management', 426),\n",
       " ('network', 416),\n",
       " ('technology', 375),\n",
       " ('sentiment analysis', 364),\n",
       " ('artificial neural network', 346),\n",
       " ('remote sensing', 339),\n",
       " ('covid 19', 336),\n",
       " ('data mining', 323),\n",
       " ('framework', 315)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ZOU/Desktop/code/visualize/All_Keywords_Cloud.html'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a word cloud based on All Keywords\n",
    "\n",
    "import pyecharts.options as opts\n",
    "from pyecharts.charts import WordCloud\n",
    "\n",
    "# plot the word cloud\n",
    "(\n",
    "    WordCloud()\n",
    "    .add(series_name=\"\", data_pair=word_freq[:600], word_size_range=[6, 66])\n",
    "    .set_global_opts(\n",
    "        tooltip_opts=opts.TooltipOpts(is_show=True),\n",
    "    )\n",
    "    .render(\"visualize/All_Keywords_Cloud.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty All Keywords: 1115\n"
     ]
    }
   ],
   "source": [
    "# check how many empty list All Keywords\n",
    "print('Number of empty All Keywords:', df['All Keywords'].apply(lambda x: len(x) == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a co-occurence network of All Keywords for each group\n",
    "# Save network files to json for VosViewer\n",
    "import networkx as nx\n",
    "import nx2vos\n",
    "\n",
    "for group in range(1, 9):\n",
    "    # build a co-occurrence network of All Keywords for each group\n",
    "    # each row of the column 'All Keywords' is a list of keywords\n",
    "    G = nx.Graph()\n",
    "    for keywords in df[df['Group'] == group]['All Keywords']:\n",
    "        for i in range(len(keywords)):\n",
    "            for j in range(i+1, len(keywords)):\n",
    "                if G.has_edge(keywords[i], keywords[j]):\n",
    "                    G[keywords[i]][keywords[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(keywords[i], keywords[j], weight=1)\n",
    "\n",
    "    # Save network files to json for VosViewer\n",
    "    nx2vos.write_vos_json(G, f'output/ALL/All_Keywords_G{group}.json')\n",
    "\n",
    "    # save link in txt in weight descending order\n",
    "    sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "    with open(f'output/ALL/All_Keywords_G{group}_links.txt', 'w') as f:\n",
    "        for edge in sorted_edges:\n",
    "            f.write(f\"{edge[0]}\\t{edge[1]}\\t{edge[2]['weight']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the txt file\n",
    "# show the links all the group have in common\n",
    "# common_links = []\n",
    "# for group in range(1, 9):\n",
    "#     with open(f'output/ALL/All_Keywords_G{group}_links.txt', 'r') as f:\n",
    "#         links = set()\n",
    "#         for line in f:\n",
    "#             links.add(line.split('\\t')[0] + '\\t' + line.split('\\t')[1])\n",
    "#         if group == 1:\n",
    "#             common_links = links\n",
    "#         else:\n",
    "#             common_links = common_links.intersection(links)\n",
    "# common_links\n",
    "# len(common_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-build the network for each group by removing the common links\n",
    "# for group in range(1, 9):\n",
    "#     # build a co-occurrence network of Author Keywords for each group\n",
    "#     # each row of the column 'All Keywords' is a list of keywords\n",
    "#     G = nx.Graph()\n",
    "#     for keywords in df[df['Group'] == group]['All Keywords']:\n",
    "#         keywords = eval(keywords)\n",
    "#         for i in range(len(keywords)):\n",
    "#             for j in range(i+1, len(keywords)):\n",
    "#                 if G.has_edge(keywords[i], keywords[j]):\n",
    "#                     G[keywords[i]][keywords[j]]['weight'] += 1\n",
    "#                 else:\n",
    "#                     G.add_edge(keywords[i], keywords[j], weight=1)\n",
    "\n",
    "#     # remove the common links\n",
    "#     for link in common_links:\n",
    "#         G.remove_edge(link.split('\\t')[0], link.split('\\t')[1])\n",
    "\n",
    "#     # Save network files to json for VosViewer\n",
    "#     nx2vos.write_vos_json(G, f'output/ALL/All_Keywords_G{group}_no_common.json')\n",
    "\n",
    "#     # save link in txt in weight descending order\n",
    "#     sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "#     with open(f'output/ALL/All_Keywords_G{group}_no_common_links.txt', 'w') as f:\n",
    "#         for edge in sorted_edges:\n",
    "#             f.write(f\"{edge[0]}\\t{edge[1]}\\t{edge[2]['weight']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Keywords Plus lemmatized'] = df['Keywords Plus lemmatized'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty Keywords Plus lemmatized: 5619\n"
     ]
    }
   ],
   "source": [
    "# check how many empty list Keywords Plus lemmatized\n",
    "print('Number of empty Keywords Plus lemmatized:', df['Keywords Plus lemmatized'].apply(lambda x: len(x) == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for Keywords Plus lemmatized\n",
    "for group in range(1, 9):\n",
    "    # build a co-occurrence network of Keywords Plus lemmatized for each group\n",
    "    # each row of the column 'All Keywords' is a list of keywords\n",
    "    G = nx.Graph()\n",
    "    for keywords in df[df['Group'] == group]['Keywords Plus lemmatized']:\n",
    "        for i in range(len(keywords)):\n",
    "            for j in range(i+1, len(keywords)):\n",
    "                if G.has_edge(keywords[i], keywords[j]):\n",
    "                    G[keywords[i]][keywords[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(keywords[i], keywords[j], weight=1)\n",
    "\n",
    "    # Save network files to json for VosViewer\n",
    "    nx2vos.write_vos_json(G, f'output/KP/Keywords_Plus_G{group}.json')\n",
    "\n",
    "    # save link in txt in weight descending order\n",
    "    sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "    with open(f'output/KP/Keywords_Plus_G{group}_links.txt', 'w') as f:\n",
    "        for edge in sorted_edges:\n",
    "            f.write(f\"{edge[0]}\\t{edge[1]}\\t{edge[2]['weight']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Author Keywords lemmatized'] = df['Author Keywords lemmatized'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty Author Keywords lemmatized: 2327\n"
     ]
    }
   ],
   "source": [
    "# check how many empty list Author Keywords lemmatized\n",
    "print('Number of empty Author Keywords lemmatized:', df['Author Keywords lemmatized'].apply(lambda x: len(x) == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for Author Keywords lemmatized\n",
    "for group in range(1, 9):\n",
    "    # build a co-occurrence network of Author Keywords lemmatized for each group\n",
    "    # each row of the column 'Author Keywords' is a list of keywords\n",
    "    G = nx.Graph()\n",
    "    for keywords in df[df['Group'] == group]['Author Keywords lemmatized']:\n",
    "        for i in range(len(keywords)):\n",
    "            for j in range(i+1, len(keywords)):\n",
    "                if G.has_edge(keywords[i], keywords[j]):\n",
    "                    G[keywords[i]][keywords[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(keywords[i], keywords[j], weight=1)\n",
    "\n",
    "    # Save network files to json for VosViewer\n",
    "    nx2vos.write_vos_json(G, f'output/AK/Author_Keywords_G{group}.json')\n",
    "\n",
    "    # save link in txt in weight descending order\n",
    "    sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "    with open(f'output/AK/Author_Keywords_G{group}_links.txt', 'w') as f:\n",
    "        for edge in sorted_edges:\n",
    "            f.write(f\"{edge[0]}\\t{edge[1]}\\t{edge[2]['weight']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
